{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is stolen from \"https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    # Row-normalizes the attricutes to sum up to one\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=.05, num_test=.1, is_undirected=True,\n",
    "                      # if split_labels is to true it will split positive and negative labels,\n",
    "                      # and save them in distinct attributes\n",
    "                      split_labels=True, \n",
    "                      # Add_negative_train_samples: Whether to add negative training samples for link\n",
    "                      # prediction for link prediction. negative train samples might e.g. be edges that\n",
    "                      # are not suposed to be in the graph.\n",
    "                      add_negative_train_samples=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data/preprocessed/\n",
      "..\\data/preprocessed/meshgraphnets_miniset30traj5ts_vis.pt\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Preprocessed data \"\"\"\n",
    "dataset_dir = os.path.join(os.path.pardir, 'data/preprocessed/')\n",
    "print(dataset_dir)\n",
    "file_path=os.path.join(dataset_dir, 'meshgraphnets_miniset30traj5ts_vis.pt')\n",
    "print(file_path)\n",
    "dataset_full_timesteps = torch.load(file_path)\n",
    "dataset = torch.load(file_path)[:1]\n",
    "data2 = torch.load(file_path)[1:2]\n",
    "print(len(dataset_full_timesteps)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1923, 11], edge_index=[2, 9412], edge_attr=[9412, 3], y=[1923, 2], p=[1923, 1], pos_edge_label=[4706], pos_edge_label_index=[2, 4706])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=transform)\n",
    "pls = transform(dataset)\n",
    "train_data, val_data, test_data = pls[0]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        # Mu is the difference between the GCNE and the VGCNE\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = dataset.num_features, 16\n",
    "variation = \"GAE\"\n",
    "if variation == \"GAE\":\n",
    "    model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "elif variation == \"VGAE\":\n",
    "    model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "else:\n",
    "    raise Exception(\"Model type not specified\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "\n",
    "    # Only relevant if we use the variational graph auto encoder\n",
    "    if variation == \"VGAE\":\n",
    "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Can we remove float?\n",
    "    return float(loss)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, AUC: 0.6576, AP: 0.6994\n",
      "Epoch: 002, AUC: 0.6525, AP: 0.6956\n",
      "Epoch: 003, AUC: 0.6515, AP: 0.6952\n",
      "Epoch: 004, AUC: 0.6535, AP: 0.6983\n",
      "Epoch: 005, AUC: 0.6549, AP: 0.7052\n",
      "Epoch: 006, AUC: 0.6577, AP: 0.7144\n",
      "Epoch: 007, AUC: 0.6613, AP: 0.7224\n",
      "Epoch: 008, AUC: 0.6622, AP: 0.7253\n",
      "Epoch: 009, AUC: 0.6624, AP: 0.7255\n",
      "Epoch: 010, AUC: 0.6618, AP: 0.7242\n",
      "Epoch: 011, AUC: 0.6635, AP: 0.7243\n",
      "Epoch: 012, AUC: 0.6731, AP: 0.7280\n",
      "Epoch: 013, AUC: 0.7053, AP: 0.7424\n",
      "Epoch: 014, AUC: 0.7548, AP: 0.7716\n",
      "Epoch: 015, AUC: 0.7820, AP: 0.7919\n",
      "Epoch: 016, AUC: 0.7910, AP: 0.7984\n",
      "Epoch: 017, AUC: 0.7922, AP: 0.7990\n",
      "Epoch: 018, AUC: 0.7919, AP: 0.7977\n",
      "Epoch: 019, AUC: 0.7963, AP: 0.8007\n",
      "Epoch: 020, AUC: 0.7967, AP: 0.7999\n",
      "Epoch: 021, AUC: 0.7949, AP: 0.7971\n",
      "Epoch: 022, AUC: 0.8001, AP: 0.8007\n",
      "Epoch: 023, AUC: 0.8053, AP: 0.8046\n",
      "Epoch: 024, AUC: 0.8086, AP: 0.8081\n",
      "Epoch: 025, AUC: 0.8111, AP: 0.8120\n",
      "Epoch: 026, AUC: 0.8195, AP: 0.8203\n",
      "Epoch: 027, AUC: 0.8300, AP: 0.8294\n",
      "Epoch: 028, AUC: 0.8400, AP: 0.8378\n",
      "Epoch: 029, AUC: 0.8473, AP: 0.8446\n",
      "Epoch: 030, AUC: 0.8516, AP: 0.8497\n",
      "Epoch: 031, AUC: 0.8539, AP: 0.8524\n",
      "Epoch: 032, AUC: 0.8562, AP: 0.8523\n",
      "Epoch: 033, AUC: 0.8574, AP: 0.8510\n",
      "Epoch: 034, AUC: 0.8574, AP: 0.8500\n",
      "Epoch: 035, AUC: 0.8565, AP: 0.8485\n",
      "Epoch: 036, AUC: 0.8555, AP: 0.8468\n",
      "Epoch: 037, AUC: 0.8554, AP: 0.8458\n",
      "Epoch: 038, AUC: 0.8563, AP: 0.8459\n",
      "Epoch: 039, AUC: 0.8576, AP: 0.8473\n",
      "Epoch: 040, AUC: 0.8585, AP: 0.8483\n",
      "Epoch: 041, AUC: 0.8594, AP: 0.8496\n",
      "Epoch: 042, AUC: 0.8593, AP: 0.8513\n",
      "Epoch: 043, AUC: 0.8584, AP: 0.8518\n",
      "Epoch: 044, AUC: 0.8570, AP: 0.8513\n",
      "Epoch: 045, AUC: 0.8571, AP: 0.8511\n",
      "Epoch: 046, AUC: 0.8565, AP: 0.8504\n",
      "Epoch: 047, AUC: 0.8555, AP: 0.8501\n",
      "Epoch: 048, AUC: 0.8539, AP: 0.8495\n",
      "Epoch: 049, AUC: 0.8529, AP: 0.8493\n",
      "Epoch: 050, AUC: 0.8525, AP: 0.8492\n",
      "Epoch: 051, AUC: 0.8526, AP: 0.8490\n",
      "Epoch: 052, AUC: 0.8530, AP: 0.8492\n",
      "Epoch: 053, AUC: 0.8534, AP: 0.8498\n",
      "Epoch: 054, AUC: 0.8534, AP: 0.8503\n",
      "Epoch: 055, AUC: 0.8537, AP: 0.8508\n",
      "Epoch: 056, AUC: 0.8538, AP: 0.8513\n",
      "Epoch: 057, AUC: 0.8540, AP: 0.8516\n",
      "Epoch: 058, AUC: 0.8546, AP: 0.8523\n",
      "Epoch: 059, AUC: 0.8554, AP: 0.8525\n",
      "Epoch: 060, AUC: 0.8558, AP: 0.8523\n",
      "Epoch: 061, AUC: 0.8558, AP: 0.8528\n",
      "Epoch: 062, AUC: 0.8546, AP: 0.8528\n",
      "Epoch: 063, AUC: 0.8537, AP: 0.8522\n",
      "Epoch: 064, AUC: 0.8546, AP: 0.8526\n",
      "Epoch: 065, AUC: 0.8552, AP: 0.8523\n",
      "Epoch: 066, AUC: 0.8551, AP: 0.8520\n",
      "Epoch: 067, AUC: 0.8546, AP: 0.8518\n",
      "Epoch: 068, AUC: 0.8538, AP: 0.8515\n",
      "Epoch: 069, AUC: 0.8536, AP: 0.8517\n",
      "Epoch: 070, AUC: 0.8538, AP: 0.8519\n",
      "Epoch: 071, AUC: 0.8536, AP: 0.8519\n",
      "Epoch: 072, AUC: 0.8534, AP: 0.8518\n",
      "Epoch: 073, AUC: 0.8531, AP: 0.8518\n",
      "Epoch: 074, AUC: 0.8526, AP: 0.8514\n",
      "Epoch: 075, AUC: 0.8524, AP: 0.8513\n",
      "Epoch: 076, AUC: 0.8516, AP: 0.8509\n",
      "Epoch: 077, AUC: 0.8508, AP: 0.8504\n",
      "Epoch: 078, AUC: 0.8504, AP: 0.8504\n",
      "Epoch: 079, AUC: 0.8496, AP: 0.8499\n",
      "Epoch: 080, AUC: 0.8494, AP: 0.8497\n",
      "Epoch: 081, AUC: 0.8495, AP: 0.8493\n",
      "Epoch: 082, AUC: 0.8490, AP: 0.8489\n",
      "Epoch: 083, AUC: 0.8487, AP: 0.8494\n",
      "Epoch: 084, AUC: 0.8476, AP: 0.8488\n",
      "Epoch: 085, AUC: 0.8479, AP: 0.8488\n",
      "Epoch: 086, AUC: 0.8488, AP: 0.8485\n",
      "Epoch: 087, AUC: 0.8485, AP: 0.8476\n",
      "Epoch: 088, AUC: 0.8484, AP: 0.8476\n",
      "Epoch: 089, AUC: 0.8480, AP: 0.8481\n",
      "Epoch: 090, AUC: 0.8470, AP: 0.8474\n",
      "Epoch: 091, AUC: 0.8474, AP: 0.8481\n",
      "Epoch: 092, AUC: 0.8475, AP: 0.8481\n",
      "Epoch: 093, AUC: 0.8478, AP: 0.8481\n",
      "Epoch: 094, AUC: 0.8481, AP: 0.8484\n",
      "Epoch: 095, AUC: 0.8479, AP: 0.8486\n",
      "Epoch: 096, AUC: 0.8475, AP: 0.8479\n",
      "Epoch: 097, AUC: 0.8471, AP: 0.8479\n",
      "Epoch: 098, AUC: 0.8469, AP: 0.8479\n",
      "Epoch: 099, AUC: 0.8462, AP: 0.8470\n",
      "Epoch: 100, AUC: 0.8461, AP: 0.8468\n",
      "Median time per epoch: 0.0839)s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "times = []\n",
    "epochs = 100\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start = time.time()\n",
    "    loss = train()\n",
    "    auc, ap = test(test_data)\n",
    "    print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f})s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
